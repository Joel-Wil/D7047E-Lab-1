{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9d179807-bad8-4e2e-8fe8-abe5d0f2d0bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2025-04-24 15:55:26.607679: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1745502926.627521     154 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1745502926.633646     154 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1745502926.649649     154 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1745502926.649667     154 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1745502926.649669     154 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1745502926.649671     154 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "2025-04-24 15:55:26.654452: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/4, Train Loss: 0.8530\n",
      "Validation Loss: 0.5817\n",
      "Best model saved.\n",
      "Epoch 2/4, Train Loss: 0.4606\n",
      "Validation Loss: 0.4424\n",
      "Best model saved.\n",
      "Epoch 3/4, Train Loss: 0.3064\n",
      "Validation Loss: 0.4381\n",
      "Best model saved.\n",
      "Epoch 4/4, Train Loss: 0.2144\n",
      "Validation Loss: 0.4575\n",
      "\n",
      "Test Results:\n",
      "Accuracy: 0.8309636650868878\n",
      "Precision: 0.8416191977029783\n",
      "Recall: 0.8347038076661489\n",
      "\n",
      "Classification Report:\n",
      "                     precision    recall  f1-score   support\n",
      "\n",
      "Extremely Negative       0.87      0.82      0.85       592\n",
      "          Negative       0.82      0.82      0.82      1041\n",
      "           Neutral       0.92      0.84      0.88       619\n",
      "          Positive       0.78      0.81      0.79       947\n",
      "Extremely Positive       0.83      0.88      0.85       599\n",
      "\n",
      "          accuracy                           0.83      3798\n",
      "         macro avg       0.84      0.83      0.84      3798\n",
      "      weighted avg       0.83      0.83      0.83      3798\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from transformers import DistilBertTokenizerFast, DistilBertModel\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, classification_report\n",
    "\n",
    "# Constants\n",
    "LABEL_MAPPING = {\n",
    "    \"Extremely Negative\": 0,\n",
    "    \"Negative\": 1,\n",
    "    \"Neutral\": 2,\n",
    "    \"Positive\": 3,\n",
    "    \"Extremely Positive\": 4\n",
    "}\n",
    "NUM_LABELS = len(LABEL_MAPPING)\n",
    "BATCH_SIZE = 16\n",
    "EPOCHS = 4\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Dataset\n",
    "class SentimentDataset(Dataset):\n",
    "    def __init__(self, texts, labels, tokenizer, max_len=128):\n",
    "        self.encodings = tokenizer(texts, padding=True, truncation=True, max_length=max_len, return_tensors=\"pt\")\n",
    "        self.labels = torch.tensor(labels)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return {\n",
    "            \"input_ids\": self.encodings[\"input_ids\"][idx],\n",
    "            \"attention_mask\": self.encodings[\"attention_mask\"][idx],\n",
    "            \"labels\": self.labels[idx]\n",
    "        }\n",
    "\n",
    "# Classifier Head\n",
    "class SentimentClassifier(nn.Module):\n",
    "    def __init__(self, hidden_size=768, num_classes=NUM_LABELS):\n",
    "        super(SentimentClassifier, self).__init__()\n",
    "        self.bert = DistilBertModel.from_pretrained(\"distilbert-base-uncased\")\n",
    "        self.dropout = nn.Dropout(0.3)\n",
    "        self.classifier = nn.Linear(hidden_size, num_classes)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        pooled_output = outputs.last_hidden_state[:, 0]  # CLS token representation\n",
    "        pooled_output = self.dropout(pooled_output)\n",
    "        return self.classifier(pooled_output)\n",
    "\n",
    "# Load & preprocess\n",
    "def load_and_prepare_data(file_path):\n",
    "    df = pd.read_csv(file_path, encoding='latin1', on_bad_lines='skip')\n",
    "    df.columns = df.columns.str.strip()\n",
    "    df = df[['OriginalTweet', 'Sentiment']]\n",
    "    df.columns = ['text', 'label']\n",
    "    df['label'] = df['label'].map(LABEL_MAPPING)\n",
    "    return df.dropna()\n",
    "\n",
    "train_df = load_and_prepare_data(\"Corona_NLP_train.csv\")\n",
    "test_df = load_and_prepare_data(\"Corona_NLP_test.csv\")\n",
    "\n",
    "train_texts, val_texts, train_labels, val_labels = train_test_split(\n",
    "    train_df[\"text\"].tolist(),\n",
    "    train_df[\"label\"].tolist(),\n",
    "    test_size=0.375,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "tokenizer = DistilBertTokenizerFast.from_pretrained(\"distilbert-base-uncased\")\n",
    "\n",
    "train_dataset = SentimentDataset(train_texts, train_labels, tokenizer)\n",
    "val_dataset = SentimentDataset(val_texts, val_labels, tokenizer)\n",
    "test_dataset = SentimentDataset(test_df[\"text\"].tolist(), test_df[\"label\"].tolist(), tokenizer)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE)\n",
    "\n",
    "# Model, Loss, Optimizer\n",
    "model = SentimentClassifier().to(DEVICE)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.AdamW(model.parameters(), lr=2e-5)\n",
    "\n",
    "# Training Loop\n",
    "best_val_loss = float('inf')\n",
    "for epoch in range(EPOCHS):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    for batch in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        input_ids = batch[\"input_ids\"].to(DEVICE)\n",
    "        attention_mask = batch[\"attention_mask\"].to(DEVICE)\n",
    "        labels = batch[\"labels\"].to(DEVICE)\n",
    "\n",
    "        outputs = model(input_ids, attention_mask)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        train_loss += loss.item()\n",
    "\n",
    "    avg_train_loss = train_loss / len(train_loader)\n",
    "    print(f\"Epoch {epoch+1}/{EPOCHS}, Train Loss: {avg_train_loss:.4f}\")\n",
    "\n",
    "    # Validation\n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "    val_preds, val_true = [], []\n",
    "    with torch.no_grad():\n",
    "        for batch in val_loader:\n",
    "            input_ids = batch[\"input_ids\"].to(DEVICE)\n",
    "            attention_mask = batch[\"attention_mask\"].to(DEVICE)\n",
    "            labels = batch[\"labels\"].to(DEVICE)\n",
    "\n",
    "            outputs = model(input_ids, attention_mask)\n",
    "            loss = criterion(outputs, labels)\n",
    "            val_loss += loss.item()\n",
    "\n",
    "            preds = torch.argmax(outputs, dim=1)\n",
    "            val_preds.extend(preds.cpu().numpy())\n",
    "            val_true.extend(labels.cpu().numpy())\n",
    "\n",
    "    avg_val_loss = val_loss / len(val_loader)\n",
    "    print(f\"Validation Loss: {avg_val_loss:.4f}\")\n",
    "\n",
    "    if avg_val_loss < best_val_loss:\n",
    "        best_val_loss = avg_val_loss\n",
    "        best_model_state = model.state_dict().copy()\n",
    "        print(\"Best model saved.\")\n",
    "\n",
    "# Testing\n",
    "model.load_state_dict(best_model_state)\n",
    "model.eval()\n",
    "test_preds, test_true = [], []\n",
    "with torch.no_grad():\n",
    "    for batch in test_loader:\n",
    "        input_ids = batch[\"input_ids\"].to(DEVICE)\n",
    "        attention_mask = batch[\"attention_mask\"].to(DEVICE)\n",
    "        labels = batch[\"labels\"].to(DEVICE)\n",
    "\n",
    "        outputs = model(input_ids, attention_mask)\n",
    "        preds = torch.argmax(outputs, dim=1)\n",
    "        test_preds.extend(preds.cpu().numpy())\n",
    "        test_true.extend(labels.cpu().numpy())\n",
    "\n",
    "print(\"\\nTest Results:\")\n",
    "print(\"Accuracy:\", accuracy_score(test_true, test_preds))\n",
    "print(\"Precision:\", precision_score(test_true, test_preds, average='macro'))\n",
    "print(\"Recall:\", recall_score(test_true, test_preds, average='macro'))\n",
    "print(\"\\nClassification Report:\\n\", classification_report(test_true, test_preds, target_names=LABEL_MAPPING.keys()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d49379c5-dded-4a8f-9112-b07f4bbf8e94",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9c6a7e2e-4749-499b-baeb-c366f8a64c28",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/50], Training Loss: 1.2493\n",
      "Validation Loss: 1.0753\n",
      "Best model updated.\n",
      "Epoch [2/50], Training Loss: 0.8784\n",
      "Validation Loss: 1.0554\n",
      "Best model updated.\n",
      "Epoch [3/50], Training Loss: 0.6700\n",
      "Validation Loss: 1.1124\n",
      "No improvement. Patience left: 4\n",
      "Epoch [4/50], Training Loss: 0.5142\n",
      "Validation Loss: 1.2479\n",
      "No improvement. Patience left: 3\n",
      "Epoch [5/50], Training Loss: 0.4207\n",
      "Validation Loss: 1.3743\n",
      "No improvement. Patience left: 2\n",
      "Epoch [6/50], Training Loss: 0.3406\n",
      "Validation Loss: 1.4630\n",
      "No improvement. Patience left: 1\n",
      "Epoch [7/50], Training Loss: 0.3000\n",
      "Validation Loss: 1.5615\n",
      "No improvement. Patience left: 0\n",
      "\n",
      "Early stopping triggered.\n",
      "\n",
      "Loaded best model for testing.\n",
      "\n",
      "Test Results:\n",
      "Accuracy: 0.5194839389152185\n",
      "Precision: 0.5327908842939264\n",
      "Recall: 0.5344947882651091\n",
      "\n",
      "Classification Report:\n",
      "                     precision    recall  f1-score   support\n",
      "\n",
      "Extremely Negative       0.60      0.52      0.56       592\n",
      "          Negative       0.48      0.43      0.45      1041\n",
      "           Neutral       0.53      0.58      0.55       619\n",
      "          Positive       0.46      0.50      0.48       947\n",
      "Extremely Positive       0.60      0.64      0.62       599\n",
      "\n",
      "          accuracy                           0.52      3798\n",
      "         macro avg       0.53      0.53      0.53      3798\n",
      "      weighted avg       0.52      0.52      0.52      3798\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import word_tokenize\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, classification_report\n",
    "\n",
    "# Ensure NLTK resources are available\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "LABEL_MAPPING = {\n",
    "    \"Extremely Negative\": 0,\n",
    "    \"Negative\": 1,\n",
    "    \"Neutral\": 2,\n",
    "    \"Positive\": 3,\n",
    "    \"Extremely Positive\": 4\n",
    "}\n",
    "\n",
    "def preprocess_pandas(data):\n",
    "    data['Sentence'] = data['Sentence'].str.lower()\n",
    "    data['Sentence'] = data['Sentence'].replace(r'[a-zA-Z0-9-_.]+@[a-zA-Z0-9-_.]+', '', regex=True)\n",
    "    data['Sentence'] = data['Sentence'].replace(r'((25[0-5]|2[0-4][0-9]|[01]?[0-9][0-9]?)(\\.|$)){4}', '', regex=True)\n",
    "    data['Sentence'] = data['Sentence'].str.replace(r'[^\\w\\s]', '', regex=True)\n",
    "    data['Sentence'] = data['Sentence'].replace(r'\\d', '', regex=True)\n",
    "\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    data['Sentence'] = data['Sentence'].apply(lambda x: \" \".join(\n",
    "        [word for word in word_tokenize(x) if word not in stop_words]\n",
    "    ))\n",
    "    return data\n",
    "\n",
    "class SentimentANN(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim=128, dropout=0.3):\n",
    "        super(SentimentANN, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
    "        self.bn1 = nn.BatchNorm1d(hidden_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.fc2 = nn.Linear(hidden_dim, 64)\n",
    "        self.bn2 = nn.BatchNorm1d(64)\n",
    "        self.fc3 = nn.Linear(64, 5)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.bn1(self.fc1(x)))\n",
    "        x = self.dropout(x)\n",
    "        x = F.relu(self.bn2(self.fc2(x)))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Load and preprocess training data\n",
    "    train_df = pd.read_csv(\"Corona_NLP_train.csv\", encoding='latin1', on_bad_lines='skip')\n",
    "    train_df.columns = train_df.columns.str.strip()\n",
    "    train_df = train_df[['OriginalTweet', 'Sentiment']]\n",
    "    train_df.columns = ['Sentence', 'Class']\n",
    "    train_df['Class'] = train_df['Class'].map(LABEL_MAPPING)\n",
    "    train_df = train_df.dropna()\n",
    "    train_df = preprocess_pandas(train_df)\n",
    "\n",
    "    # Load and preprocess test data\n",
    "    test_df = pd.read_csv(\"Corona_NLP_test.csv\", encoding='latin1', on_bad_lines='skip')\n",
    "    test_df.columns = test_df.columns.str.strip()\n",
    "    test_df = test_df[['OriginalTweet', 'Sentiment']]\n",
    "    test_df.columns = ['Sentence', 'Class']\n",
    "    test_df['Class'] = test_df['Class'].map(LABEL_MAPPING)\n",
    "    test_df = test_df.dropna()\n",
    "    test_df = preprocess_pandas(test_df)\n",
    "\n",
    "    # Train/Val Split\n",
    "    train_texts, val_texts, train_labels, val_labels = train_test_split(\n",
    "        train_df['Sentence'].values.astype('U'),\n",
    "        train_df['Class'].values.astype('int32'),\n",
    "        test_size=0.375,  # 30% val\n",
    "        random_state=0,\n",
    "        shuffle=True\n",
    "    )\n",
    "\n",
    "    test_texts = test_df['Sentence'].values.astype('U')\n",
    "    test_labels = test_df['Class'].values.astype('int32')\n",
    "\n",
    "    # TF-IDF and Scaling\n",
    "    vectorizer = TfidfVectorizer(analyzer='word', ngram_range=(1,2), max_features=10000, max_df=0.5, use_idf=True, norm='l2')\n",
    "    train_vec = vectorizer.fit_transform(train_texts).toarray()\n",
    "    val_vec = vectorizer.transform(val_texts).toarray()\n",
    "    test_vec = vectorizer.transform(test_texts).toarray()\n",
    "\n",
    "    scaler = MinMaxScaler()\n",
    "    train_vec = scaler.fit_transform(train_vec)\n",
    "    val_vec = scaler.transform(val_vec)\n",
    "    test_vec = scaler.transform(test_vec)\n",
    "\n",
    "    # Tensor conversion\n",
    "    train_x_tensor = torch.tensor(train_vec, dtype=torch.float32)\n",
    "    train_y_tensor = torch.tensor(train_labels, dtype=torch.long)\n",
    "    val_x_tensor = torch.tensor(val_vec, dtype=torch.float32)\n",
    "    val_y_tensor = torch.tensor(val_labels, dtype=torch.long)\n",
    "    test_x_tensor = torch.tensor(test_vec, dtype=torch.float32)\n",
    "    test_y_tensor = torch.tensor(test_labels, dtype=torch.long)\n",
    "\n",
    "    input_dim = train_x_tensor.shape[1]\n",
    "    model = SentimentANN(input_dim)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "    batch_size = 16\n",
    "    train_loader = DataLoader(TensorDataset(train_x_tensor, train_y_tensor), batch_size=batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(TensorDataset(val_x_tensor, val_y_tensor), batch_size=batch_size)\n",
    "    test_loader = DataLoader(TensorDataset(test_x_tensor, test_y_tensor), batch_size=batch_size)\n",
    "\n",
    "    num_epochs = 50\n",
    "    patience = 5\n",
    "    best_val_loss = float('inf')\n",
    "    best_model_state = None\n",
    "    epochs_no_improve = 0\n",
    "\n",
    "    # Training loop\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        for batch_x, batch_y in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(batch_x)\n",
    "            loss = criterion(outputs, batch_y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "\n",
    "        avg_train_loss = total_loss / len(train_loader)\n",
    "        print(f\"Epoch [{epoch+1}/{num_epochs}], Training Loss: {avg_train_loss:.4f}\")\n",
    "\n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        val_preds, val_true = [], []\n",
    "        \n",
    "        # Validation loop\n",
    "        with torch.no_grad():\n",
    "            for val_x, val_y in val_loader:\n",
    "                outputs = model(val_x)\n",
    "                loss = criterion(outputs, val_y)\n",
    "                val_loss += loss.item()\n",
    "                _, predicted = torch.max(outputs, 1)\n",
    "                val_preds.extend(predicted.tolist())\n",
    "                val_true.extend(val_y.tolist())\n",
    "\n",
    "        avg_val_loss = val_loss / len(val_loader)\n",
    "        print(f\"Validation Loss: {avg_val_loss:.4f}\")\n",
    "\n",
    "        if avg_val_loss < best_val_loss:\n",
    "            best_val_loss = avg_val_loss\n",
    "            best_model_state = model.state_dict().copy()\n",
    "            epochs_no_improve = 0\n",
    "            print(\"Best model updated.\")\n",
    "        else:\n",
    "            epochs_no_improve += 1\n",
    "            print(f\"No improvement. Patience left: {patience - epochs_no_improve}\")\n",
    "\n",
    "        if epochs_no_improve >= patience:\n",
    "            print(\"\\nEarly stopping triggered.\")\n",
    "            break\n",
    "\n",
    "    # Load best model\n",
    "    if best_model_state:\n",
    "        model.load_state_dict(best_model_state)\n",
    "        print(\"\\nLoaded best model for testing.\")\n",
    "\n",
    "    # Testing\n",
    "    model.eval()\n",
    "    test_preds, test_true = [], []\n",
    "    with torch.no_grad():\n",
    "        for test_x, test_y in test_loader:\n",
    "            outputs = model(test_x)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            test_preds.extend(predicted.tolist())\n",
    "            test_true.extend(test_y.tolist())\n",
    "\n",
    "    print(\"\\nTest Results:\")\n",
    "    print(\"Accuracy:\", accuracy_score(test_true, test_preds))\n",
    "    print(\"Precision:\", precision_score(test_true, test_preds, average='macro'))\n",
    "    print(\"Recall:\", recall_score(test_true, test_preds, average='macro'))\n",
    "    print(\"\\nClassification Report:\\n\", classification_report(test_true, test_preds, target_names=LABEL_MAPPING.keys()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e84ec4ab-2e90-464f-9bab-019287fa677b",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

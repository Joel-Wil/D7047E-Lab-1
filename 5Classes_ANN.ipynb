{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b3608372-ca2c-4b07-ae29-a3c1979688af",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\joelw\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\joelw\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10], Loss: 19.9907\n",
      "Epoch [2/10], Loss: 18.1466\n",
      "Epoch [3/10], Loss: 13.6885\n",
      "Epoch [4/10], Loss: 8.0774\n",
      "Epoch [5/10], Loss: 4.3439\n",
      "Epoch [6/10], Loss: 2.4600\n",
      "Epoch [7/10], Loss: 1.5705\n",
      "Epoch [8/10], Loss: 1.0881\n",
      "Epoch [9/10], Loss: 0.7931\n",
      "Epoch [10/10], Loss: 0.6397\n",
      "\n",
      "Validation Results:\n",
      "Accuracy: 0.87\n",
      "Precision: 0.9\n",
      "Recall: 0.8490566037735849\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.84      0.89      0.87        47\n",
      "           1       0.90      0.85      0.87        53\n",
      "\n",
      "    accuracy                           0.87       100\n",
      "   macro avg       0.87      0.87      0.87       100\n",
      "weighted avg       0.87      0.87      0.87       100\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import nltk\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import word_tokenize\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, precision_score, recall_score, classification_report\n",
    "\n",
    "nltk.download('punkt_tab')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "def preprocess_pandas(data, columns):\n",
    "    processed_rows = []\n",
    "    data['Sentence'] = data['Sentence'].str.lower()\n",
    "    data['Sentence'] = data['Sentence'].replace('[a-zA-Z0-9-_.]+@[a-zA-Z0-9-_.]+', '', regex=True)                      # remove emails\n",
    "    data['Sentence'] = data['Sentence'].replace('((25[0-5]|2[0-4][0-9]|[01]?[0-9][0-9]?)(\\.|$)){4}', '', regex=True)    # remove IPs\n",
    "    data['Sentence'] = data['Sentence'].str.replace('[^\\w\\s]', '', regex=True)                                          # remove special characters\n",
    "    data['Sentence'] = data['Sentence'].replace('\\d', '', regex=True)                                                   # remove numbers\n",
    "\n",
    "    for index, row in data.iterrows():\n",
    "        word_tokens = word_tokenize(row['Sentence'])\n",
    "        filtered_sent = [w for w in word_tokens if w not in stopwords.words('english')]\n",
    "        processed_rows.append({\n",
    "            \"index\": row['index'],\n",
    "            \"Class\": row['Class'],\n",
    "            \"Sentence\": \" \".join(filtered_sent)\n",
    "        })\n",
    "\n",
    "    # Convert list of dicts to DataFrame at the end\n",
    "    return pd.DataFrame(processed_rows, columns=columns)\n",
    "\n",
    "\n",
    "# If this is the primary file that is executed (ie not an import of another file)\n",
    "if __name__ == \"__main__\":\n",
    "    # get data, pre-process and split\n",
    "    data = pd.read_csv(\"amazon_cells_labelled.txt\", delimiter='\\t', header=None)\n",
    "    data.columns = ['Sentence', 'Class']\n",
    "    data['index'] = data.index                                          # add new column index\n",
    "    columns = ['index', 'Class', 'Sentence']\n",
    "    data = preprocess_pandas(data, columns)                             # pre-process\n",
    "    training_data, validation_data, training_labels, validation_labels = train_test_split( # split the data into training, validation, and test splits\n",
    "        data['Sentence'].values.astype('U'),\n",
    "        data['Class'].values.astype('int32'),\n",
    "        test_size=0.10,\n",
    "        random_state=0,\n",
    "        shuffle=True\n",
    "    )\n",
    "\n",
    "    # vectorize data using TFIDF and transform for PyTorch for scalability\n",
    "    word_vectorizer = TfidfVectorizer(analyzer='word', ngram_range=(1,2), max_features=50000, max_df=0.5, use_idf=True, norm='l2')\n",
    "    training_data = word_vectorizer.fit_transform(training_data)        # transform texts to sparse matrix\n",
    "    training_data = training_data.todense()                             # convert to dense matrix for Pytorch\n",
    "    vocab_size = len(word_vectorizer.vocabulary_)\n",
    "    validation_data = word_vectorizer.transform(validation_data)\n",
    "    validation_data = validation_data.todense()\n",
    "    train_x_tensor = torch.from_numpy(np.array(training_data)).type(torch.FloatTensor)\n",
    "    train_y_tensor = torch.from_numpy(np.array(training_labels)).long()\n",
    "    validation_x_tensor = torch.from_numpy(np.array(validation_data)).type(torch.FloatTensor)\n",
    "    validation_y_tensor = torch.from_numpy(np.array(validation_labels)).long()\n",
    "\n",
    "\n",
    "class SentimentANN(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim=100):\n",
    "        super(SentimentANN, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
    "        self.fc2 = nn.Linear(hidden_dim, 2)  # Output: 2 klasser (positive/negative)\n",
    "\n",
    "                            \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))   #Simplifierad feedforward\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "# Initialize\n",
    "input_dim = vocab_size\n",
    "model = SentimentANN(input_dim)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Create DataLoaders\n",
    "batch_size = 32\n",
    "train_dataset = TensorDataset(train_x_tensor, train_y_tensor)\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "val_dataset = TensorDataset(validation_x_tensor, validation_y_tensor)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size)\n",
    "\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 10\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for batch_x, batch_y in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(batch_x)\n",
    "        loss = criterion(outputs, batch_y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    \n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {total_loss:.4f}\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "model.eval()\n",
    "all_preds = []\n",
    "all_labels = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for val_x, val_y in val_loader:\n",
    "        outputs = model(val_x)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        all_preds.extend(predicted.tolist())\n",
    "        all_labels.extend(val_y.tolist())\n",
    "\n",
    "\n",
    "print(\"\\nValidation Results:\")\n",
    "print(\"Accuracy:\", accuracy_score(all_labels, all_preds))\n",
    "print(\"Precision:\", precision_score(all_labels, all_preds))\n",
    "print(\"Recall:\", recall_score(all_labels, all_preds))\n",
    "print(\"\\nClassification Report:\\n\", classification_report(all_labels, all_preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4112cc3-9efd-4d86-aa44-52eeb7139452",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "005c4764-d4b7-4799-bf55-f57eb017742e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:nnlm]",
   "language": "python",
   "name": "conda-env-nnlm-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
